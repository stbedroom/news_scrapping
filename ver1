# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""
# 매일경제 뉴스 기사 검색 후 url 보여주기 #
# 가능하면 기사 원문까지도! # 

""" 1. 허핑턴 포스트 클리핑 """

# 스크래핑을 위한 requests, lxml, cssselect, html 모듈 설치
import requests
from lxml import html
from lxml import cssselect

# URL 저장 및 불러오기
URL = "http://www.huffingtonpost.kr/"
resp = requests.get(URL)

# URL을 담은 resp 에 대한 반응 체크 : 200이면 정상
resp

resp.status_code

# 허핑턴 포스트 첫페이지 전체 html 출력
print (resp.text)

type(resp.text)

type(resp)

elem = html.fromstring(resp.text)

print (elem)
type(elem)

len(elem.cssselect('div.entry h2 a'))

articles = elem.cssselect('div.entry h2 a')

type(articles)

print (articles[0].text_content().strip())

articles1 = elem.cssselect('div.entry p')
print (articles1[1])

for i, a in enumerate(articles1[0:11]):
    print (i, a.text)
    
for i, a in enumerate(articles[0]):
    print (i, a.text_content().strip())
    
for i, a in enumerate(articles[0:6]):
    content_resp = requests.get(a.get('href')) 
    print (content_resp.text.strip())
    if content_resp.status_code == 200 :
        content_elem = html.fromstring(content_resp.text)
        print (content_elem)
        
print (content_resp.text[0:200])

for i, a in enumerate(articles[0:6]):
    content_resp = requests.get(a.get('href')) 
    print (content_resp.text.strip())
    
type(content_resp.text)

print (a.text_content())

type(a.get('href'))

print (a.get('href'))

for i, a in enumerate(articles[6:20]):
    content_resp = requests.get(a.get('href')) 
    if content_resp.status_code == 200 :
        content_elem = html.fromstring(content_resp.text)
        try : 
            reporters = content_elem.cssselect('div.info span.name')
            # div.info span.name 이 뭔지 모르겠다.
            name = reporters[0].text_content().strip()
        except IndexError :
            name = 'Unknown'

        if name.startswith('|'):
            try : 
                name = reporters[0].cssselect('a')[0].text_content().strip()
            except :
                print (a.get('href'))
                continue
    else : 
        continue
    print ("{} - <<{}>> by {}".format(i, a.text_content(), name))
    
    
for i, a in enumerate(articles[3:5]):
    content_resp = requests.get(a.get('href'))
    if content_resp.status_code == 200 :
        content_elem = html.fromstring(content_resp.text)
        try : 
            reporters = content_elem.cssselect('div.info span.name')
            name = reporters[0].text_content().strip()
        except IndexError :
            name = "Unknown"
            
        if name.startswith("|"):
            try : 
                name = reporters[0].cssselect('a')[0].text_content().strip()
            except : 
                print (a.get('href'))
                continue
    else : 
        continue
    print ("{} - <{}> by {} link: {}".format(i,a.text_content(),name, a.get('href')))
    
    
for i, a in enumerate(articles):
content_resp = requests.get(a.get('href'))
print (i, a.text.strip(), a.get('href')) 


def get_reporter(article_url):
    name = 'Unknown'
    resp = requests.get(article_url)
    if resp.status_code == 200 :
        elem = html.fromstring(resp.text)
        try : 
            reporter = elem.cssselect('div.info span.name')[0]
            name = reporter.text_content().strip()
        except IndexError:
            pass
        
        if name.startswith('|'):
            try :
                name = reporter.cssselect('a')[0].text_content().strip()
            except : 
                print (a.get('href'))
                
        return name
        
for i, a in enumerate(articles[6:]):
    name = get_reporter(a.get('href'))
    print ("{} - <<{}>> by {}".format (i, a.text_content(), name))
    
resp = requests.get('http://clien.net/cs2/bbs/board.php?bo_table=park')
resp.encoding = 'utf-8'
resp

resp = requests.get("http://www.clien.net/cs2/bbs/board.php?bo_table=park")
resp.encoding = 'utf-8'
resp

elem = html.fromstring(resp.text)
posts = elem.cssselect('tr.mytr')

URL = "http://www.clien.net/cs2/bbs/board.php?bo_table=park"
for a in posts[0:10]:
    no = a.cssselect('td')[0].text_content().strip()
    title = a.cssselect('td.post_subject a ')[0].text
    link = a.cssselect('td.post_subject a')[0].get('href')
    name = a.cssselect('td.post_name')[0].text_content()
    date = a.cssselect('td')[3].text_content()
    hit = a.cssselect('td')[4].text_content()
    print ("No.{} ,제목 : {}, 게시자 : {}, 날짜 :{}, 조회수 :{}"
           .format(no, title, name, date, hit),"\n", URL + link)
           
URL = 'http://clien.net/cs2/bbs/board.php'
def clien(page):
    print(page)
    resp = requests.get(URL, params = {'page':page, 'bo_table':'park'})
    resp.encoding = 'utf8'
    
    elem = html.fromstring(resp.text)
    posts = elem.cssselect('tr.mytr')
    
    for a in posts:
        title = a.cssselect('td.post_subject')[0].text_content()
        link = a.cssselect('td.post_subject a')[0].get('href')
        aid = a.cssselect('td')[0].text_content()
        hit = a.cssselect('td')[4].text_content()
        print (aid, title, link, hit)
        
URL = "http://clien.net/cs2/bbs/board.php"
def clien(page):
    print ("page", page)
    resp = requests.get(URL, params = {"page":page, "bo_table":'park'})
    resp.encoding = 'utf8'
    
    elem = html.fromstring(resp.text)
    posts = elem.cssselect('tr.mytr')
    
    for a in posts:
        title = a.cssselect('td.post_subject')[0].text_content()
        link = a.cssselect('td.post_subject a')[0].get('href')
        no = a.cssselect('td')[0].text_content()
        name = a.cssselect('td.post_name')[0].text_content()
        hit = a.cssselect('td')[4].text_content()
        print (title, link, no, name, hit)
        
for p in range(1,3):
    clien(p)
    
for p in range(1,2) :
    clien(p)
    
import requests
from lxml import html
from lxml import cssselect

URL = 'http://epaper.mk.co.kr/PaperList.aspx'
resp = requests.get(URL)

resp

resp.status_code

elem = html.fromstring(resp.text)
len(elem.cssselect('div.paperlist ul li'))
articles = elem.cssselect('div.paperlist ul li')
print (articles)


import requests
from lxml import html
from lxml import cssselect

page = 1
while page < 8:
    try : 
        URL = 'http://epaper.mk.co.kr/PaperList.aspx?exec=&GCC=AB00699&Section=&PaperDate=&PageNo=&PageName=&CNo=&keyword=&period=&startdate=&enddate=&page=' + str(page)
        resp = requests.get(URL)
        elem = html.fromstring(resp.text)
        articles = elem.cssselect('div.paperlist ul li')

        for i, a in enumerate(articles):
            title = a.text_content().strip()
            # 광고 제거를 어떻게 하는가?
            if title == "[광고]":
                pass
            link = a.get('href')
            print (title)
            print (link)

        page += 1
    
    except :
        break
        
from selenium import webdriver
import time
from selenum.webdriver.common.keys import Keys

# 파폭 웹 드라이버 생성
driver = webdriver.Firefox()

# 주어진 웹페이지 로딩
driver.get("http://google.co.kr")

# 해당 사이트 제목이 google 인지 확인
assert "Google" in driver.title

# element 이름으로 검색창이름 확인, 파폭에서 F12
elem1 = driver.find_element_by_name("sbibod")

# 검색어 입력
elem1.send_keys("sbibod")

# 검색어 전송
elem1.submit()

# 검색결과 페이지에서 특정 문자열 포함하는 링크 텍스트 찾기
elem2 = driver.find_element_by_partial_link_text("title")

# 해당 링크 클릭
elem2.click()

# 검색 결과창 종료
driver.close()
""" <div class="sbibod " id="sfdiv">   
  <div style="height: 38px;" class="gstl_0 sbib_a"><div dir="ltr" 
  style="line-height: 38px;" id="gs_st0" class="gsst_b sbib_c"><a aria-label=
  "입력 도구" href="javascript:void(0)" class="gsst_a"><span id="gs_ok0" class=
  "gsok_a gsst_e"><img tia_property="web" tia_disable_swap="true" tia_field_name=
  "q" src="//www.gstatic.com/inputtools/images/tia.png"></span></a></div><div 
  dir="ltr" id="sb_ifc0" class="sbib_b"><div style="position: relative;" id=
  "gs_lc0"><input spellcheck="false" dir="ltr" style="border: medium none; 
  padding: 0px; margin: 0px; height: auto; width: 100%; background: transparent 
  url(&quot;data:image/gif;base64,R0lGODlhAQABAID/AMDAwAAAACH5BAEAAAAALAAAAA
  ABAAEAAAICRAEAOw%3D%3D&quot;) repeat scroll 0% 0%; position: absolute; z-
  index: 6; left: 0px; outline: medium none;" aria-autocomplete="both" r
  ole="combobox" aria-haspopup="false" class="gsfi" id="lst-ib" maxlength="
  2048" name="q" autocomplete="off" title="검색" value="추경 신속히 편성해 브
  렉시트 파고 극복" aria-label="검색" type="text"><div id="gs_sc0" style
  ="background: transparent none repeat scroll 0% 0%; color: transpar
  ent; padding: 0px; position: absolute; z-index: 2; white-space: pr
  e; visibility: hidden;" class="gsfi"></div><input dir="ltr" id="gs_
  taif0" style="border: medium none; padding: 0px; margin: 0px; heig
  ht: auto; width: 100%; position: absolute; z-index: 1; background-c
  olor: transparent; color: silver; left: 0px; visibility: hidden;" aria
  -hidden="true" autocomplete="off" disabled="" class="gsfi"><input dir=
  "ltr" id="gs_htif0" style="border: medium none; padding: 0px; margin: 0
  px; height: auto; width: 100%; position: absolute; z-index: 1; backgro
  und-color: transparent; color: silver; transition: all 0.218s ease 0
  s; opacity: 0; text-align: left; left: 0px;" aria-hidden="true" auto
  complete="off" disabled="" class="gsfi"></div></div></div>  </div>
"""

        

    
"""
URL = 'http://clien.net/cs2/bbs/board.php'
def clien(page):
    '''
    http://clien.net/cs2/bbs/board.php?bo_table=park&page=2
    '''
    print(page)
    resp = requests.get(URL, params={'page':page, 'bo_table':'park'} )
    resp.encoding = 'utf8' # clien 응답에서 encoding 정보가 빠져있음
    
    elem = html.fromstring(resp.text)
    posts = elem.cssselect('tr.mytr')
    
    for a in posts:
        title = a.cssselect('td.post_subject')[0].text_content()
        link = a.cssselect('td.post_subject a')[0].get('href')
        aid = a.cssselect('td')[0].text_content()
        hit = a.cssselect('td')[4].text_content()
        print(aid, title, link, hit)
"""    

from selenium import webdriver
import time
from selenium.webdriver.common.keys import Keys
time.sleep(1)

# 파폭 웹 드라이버 생성
driver = webdriver.Firefox()

# 주어진 웹페이지 로딩
driver.get("http://naver.com")

# 해당 사이트 제목이 google 인지 확인
assert "NAVER" in driver.title

# element 이름으로 검색창이름 확인, 파폭에서 F12
# elem1 = driver.find_element_by_id("query")
elem1 = driver.find_element_by_id("query")


# 검색어 입력
elem1.send_keys("추경 신속히 편성해 브렉시트 파고 극복")

# 검색어 전송
elem1.submit()

# 검색결과 페이지에서 특정 문자열 포함하는 링크 텍스트 찾기
elem2 = driver.find_element_by_partial_link_text("추경 신속히 편성해 브렉시트 파고 극복")

# 해당 링크 클릭
elem2.click()

# 검색 결과창 종료
driver.close()
