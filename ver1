# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""
# 매일경제 뉴스 기사 검색 후 url 보여주기 #
# 가능하면 기사 원문까지도! # 

""" 1. 허핑턴 포스트 클리핑 """

# 스크래핑을 위한 requests, lxml, cssselect, html 모듈 설치
import requests
from lxml import html
from lxml import cssselect

# URL 저장 및 불러오기
URL = "http://www.huffingtonpost.kr/"
resp = requests.get(URL)

# URL을 담은 resp 에 대한 반응 체크 : 200이면 정상
resp

resp.status_code

# 허핑턴 포스트 첫페이지 전체 html 출력
print (resp.text)

type(resp.text)

type(resp)

elem = html.fromstring(resp.text)

print (elem)
type(elem)

len(elem.cssselect('div.entry h2 a'))

articles = elem.cssselect('div.entry h2 a')

type(articles)

print (articles[0].text_content().strip())

articles1 = elem.cssselect('div.entry p')
print (articles1[1])

for i, a in enumerate(articles1[0:11]):
    print (i, a.text)
    
for i, a in enumerate(articles[0]):
    print (i, a.text_content().strip())
    
for i, a in enumerate(articles[0:6]):
    content_resp = requests.get(a.get('href')) 
    print (content_resp.text.strip())
    if content_resp.status_code == 200 :
        content_elem = html.fromstring(content_resp.text)
        print (content_elem)
        
print (content_resp.text[0:200])

for i, a in enumerate(articles[0:6]):
    content_resp = requests.get(a.get('href')) 
    print (content_resp.text.strip())
    
type(content_resp.text)

print (a.text_content())

type(a.get('href'))

print (a.get('href'))

for i, a in enumerate(articles[6:20]):
    content_resp = requests.get(a.get('href')) 
    if content_resp.status_code == 200 :
        content_elem = html.fromstring(content_resp.text)
        try : 
            reporters = content_elem.cssselect('div.info span.name')
            # div.info span.name 이 뭔지 모르겠다.
            name = reporters[0].text_content().strip()
        except IndexError :
            name = 'Unknown'

        if name.startswith('|'):
            try : 
                name = reporters[0].cssselect('a')[0].text_content().strip()
            except :
                print (a.get('href'))
                continue
    else : 
        continue
    print ("{} - <<{}>> by {}".format(i, a.text_content(), name))
    
    
for i, a in enumerate(articles[3:5]):
    content_resp = requests.get(a.get('href'))
    if content_resp.status_code == 200 :
        content_elem = html.fromstring(content_resp.text)
        try : 
            reporters = content_elem.cssselect('div.info span.name')
            name = reporters[0].text_content().strip()
        except IndexError :
            name = "Unknown"
            
        if name.startswith("|"):
            try : 
                name = reporters[0].cssselect('a')[0].text_content().strip()
            except : 
                print (a.get('href'))
                continue
    else : 
        continue
    print ("{} - <{}> by {} link: {}".format(i,a.text_content(),name, a.get('href')))
    
    
for i, a in enumerate(articles):
content_resp = requests.get(a.get('href'))
print (i, a.text.strip(), a.get('href')) 


def get_reporter(article_url):
    name = 'Unknown'
    resp = requests.get(article_url)
    if resp.status_code == 200 :
        elem = html.fromstring(resp.text)
        try : 
            reporter = elem.cssselect('div.info span.name')[0]
            name = reporter.text_content().strip()
        except IndexError:
            pass
        
        if name.startswith('|'):
            try :
                name = reporter.cssselect('a')[0].text_content().strip()
            except : 
                print (a.get('href'))
                
        return name
        
for i, a in enumerate(articles[6:]):
    name = get_reporter(a.get('href'))
    print ("{} - <<{}>> by {}".format (i, a.text_content(), name))

# 클리앙 게시판 스크래핑
resp = requests.get('http://clien.net/cs2/bbs/board.php?bo_table=park')
resp.encoding = 'utf-8'
resp

resp = requests.get("http://www.clien.net/cs2/bbs/board.php?bo_table=park")
resp.encoding = 'utf-8'
resp

elem = html.fromstring(resp.text)
posts = elem.cssselect('tr.mytr')

URL = "http://www.clien.net/cs2/bbs/board.php?bo_table=park"
for a in posts[0:10]:
    no = a.cssselect('td')[0].text_content().strip()
    title = a.cssselect('td.post_subject a ')[0].text
    link = a.cssselect('td.post_subject a')[0].get('href')
    name = a.cssselect('td.post_name')[0].text_content()
    date = a.cssselect('td')[3].text_content()
    hit = a.cssselect('td')[4].text_content()
    print ("No.{} ,제목 : {}, 게시자 : {}, 날짜 :{}, 조회수 :{}"
           .format(no, title, name, date, hit),"\n", URL + link)
           
URL = 'http://clien.net/cs2/bbs/board.php'
def clien(page):
    print(page)
    resp = requests.get(URL, params = {'page':page, 'bo_table':'park'})
    resp.encoding = 'utf8'
    
    elem = html.fromstring(resp.text)
    posts = elem.cssselect('tr.mytr')
    
    for a in posts:
        title = a.cssselect('td.post_subject')[0].text_content()
        link = a.cssselect('td.post_subject a')[0].get('href')
        aid = a.cssselect('td')[0].text_content()
        hit = a.cssselect('td')[4].text_content()
        print (aid, title, link, hit)
        
URL = "http://clien.net/cs2/bbs/board.php"
def clien(page):
    print ("page", page)
    resp = requests.get(URL, params = {"page":page, "bo_table":'park'})
    resp.encoding = 'utf8'
    
    elem = html.fromstring(resp.text)
    posts = elem.cssselect('tr.mytr')
    
    for a in posts:
        title = a.cssselect('td.post_subject')[0].text_content()
        link = a.cssselect('td.post_subject a')[0].get('href')
        no = a.cssselect('td')[0].text_content()
        name = a.cssselect('td.post_name')[0].text_content()
        hit = a.cssselect('td')[4].text_content()
        print (title, link, no, name, hit)
        
for p in range(1,3):
    clien(p)
    
for p in range(1,2) :
    clien(p)
    
import requests
from lxml import html
from lxml import cssselect

URL = 'http://epaper.mk.co.kr/PaperList.aspx'
resp = requests.get(URL)

resp

resp.status_code

elem = html.fromstring(resp.text)
len(elem.cssselect('div.paperlist ul li'))
articles = elem.cssselect('div.paperlist ul li')
print (articles)

# 매경 신문 스크래핑
import requests
from lxml import html
from lxml import cssselect

page = 1
while page < 8:
    try : 
        URL = 'http://epaper.mk.co.kr/PaperList.aspx?exec=&GCC=AB00699&Section=&PaperDate=&PageNo=&PageName=&CNo=&keyword=&period=&startdate=&enddate=&page=' + str(page)
        resp = requests.get(URL)
        elem = html.fromstring(resp.text)
        articles = elem.cssselect('div.paperlist ul li')

        for i, a in enumerate(articles):
            title = a.text_content().strip()
            # 광고 제거를 어떻게 하는가?
            if title == "[광고]":
                pass
            link = a.get('href')
            print (title)
            print (link)

        page += 1
    
    except :
        break
        

## 다시 클리앙
    
"""
URL = 'http://clien.net/cs2/bbs/board.php'
def clien(page):
    '''
    http://clien.net/cs2/bbs/board.php?bo_table=park&page=2
    '''
    print(page)
    resp = requests.get(URL, params={'page':page, 'bo_table':'park'} )
    resp.encoding = 'utf8' # clien 응답에서 encoding 정보가 빠져있음
    
    elem = html.fromstring(resp.text)
    posts = elem.cssselect('tr.mytr')
    
    for a in posts:
        title = a.cssselect('td.post_subject')[0].text_content()
        link = a.cssselect('td.post_subject a')[0].get('href')
        aid = a.cssselect('td')[0].text_content()
        hit = a.cssselect('td')[4].text_content()
        print(aid, title, link, hit)
"""    
